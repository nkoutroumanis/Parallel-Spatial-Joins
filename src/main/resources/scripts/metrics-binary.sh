    for i in 0.012 0.009 0.006 0.003; do
      spark-submit --class gr.ds.unipi.epsilongrid.MetricsReals --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 7g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m --conf spark.sql.shuffle.partitions=720 --conf spark.default.parallelism=300 ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} PARKS AREAWATER
      spark-submit --class gr.ds.unipi.twoEpsilongrid.MetricsReals --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 7g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m --conf spark.sql.shuffle.partitions=720 --conf spark.default.parallelism=300 ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} PARKS AREAWATER
      spark-submit --class gr.ds.unipi.rdd.MetricsRealsPruned --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} 4 PARKS AREAWATER 960 DIAG_PR 0.001
      spark-submit --class gr.ds.unipi.processing.MetricsRealsPruned --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} PARKS AREAWATER 960 0.001
      spark-submit --class gr.ds.unipi.stripes.MetricsRealsX --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} 4 PARKS AREAWATER 960 DIAG_PR 0.001
      spark-submit --class gr.ds.unipi.sedona.Metrics --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --jars=./libSedona/* --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} PARKS AREAWATER 960 42700000 94000000
    done

    for i in 0.012 0.009 0.006 0.003; do
      spark-submit --class gr.ds.unipi.epsilongrid.MetricsReals --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 7g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m --conf spark.sql.shuffle.partitions=720 --conf spark.default.parallelism=300 ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} BUILDINGS AREAWATER
      spark-submit --class gr.ds.unipi.twoEpsilongrid.MetricsReals --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 7g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m --conf spark.sql.shuffle.partitions=720 --conf spark.default.parallelism=300 ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} BUILDINGS AREAWATER
      spark-submit --class gr.ds.unipi.rdd.MetricsRealsPruned --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} 4 BUILDINGS AREAWATER 960 DIAG_PR 0.001
      spark-submit --class gr.ds.unipi.processing.MetricsRealsPruned --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} BUILDINGS AREAWATER 960 0.001
      if [ "$i" != "0.012" ]; then
      spark-submit --class gr.ds.unipi.stripes.MetricsRealsX --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} 4 BUILDINGS AREAWATER 960 DIAG_PR 0.001
      fi
      spark-submit --class gr.ds.unipi.sedona.Metrics --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --jars=./libSedona/* --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} BUILDINGS AREAWATER 960 58700000 94000000
    done

    for i in 0.012 0.009 0.006 0.003; do
      spark-submit --class gr.ds.unipi.epsilongrid.MetricsReals --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 7g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m --conf spark.sql.shuffle.partitions=720 --conf spark.default.parallelism=300 ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} PARKS LINEARWATER
      spark-submit --class gr.ds.unipi.twoEpsilongrid.MetricsReals --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 7g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m --conf spark.sql.shuffle.partitions=720 --conf spark.default.parallelism=300 ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} PARKS LINEARWATER
      spark-submit --class gr.ds.unipi.rdd.MetricsRealsPruned --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} 4 PARKS LINEARWATER 960 DIAG_PR 0.001
      spark-submit --class gr.ds.unipi.processing.MetricsRealsPruned --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} PARKS LINEARWATER 960 0.001
      spark-submit --class gr.ds.unipi.stripes.MetricsRealsX --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} 4 PARKS LINEARWATER 960 DIAG_PR 0.001
      spark-submit --class gr.ds.unipi.sedona.Metrics --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --jars=./libSedona/* --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} PARKS LINEARWATER 960 42700000 278000000
    done

    for i in 0.012 0.009 0.006 0.003; do
      spark-submit --class gr.ds.unipi.epsilongrid.MetricsReals --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 7g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m --conf spark.sql.shuffle.partitions=720 --conf spark.default.parallelism=300 ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} AREAWATER ROADS
      spark-submit --class gr.ds.unipi.twoEpsilongrid.MetricsReals --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 7g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m --conf spark.sql.shuffle.partitions=720 --conf spark.default.parallelism=300 ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} AREAWATER ROADS
      spark-submit --class gr.ds.unipi.rdd.MetricsRealsPruned --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} 4 AREAWATER ROADS 960 DIAG_PR 0.001
      spark-submit --class gr.ds.unipi.processing.MetricsRealsPruned --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} AREAWATER ROADS 960 0.001
      spark-submit --class gr.ds.unipi.stripes.MetricsRealsX --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} 4 AREAWATER ROADS 960 DIAG_PR 0.001
      spark-submit --class gr.ds.unipi.sedona.Metrics --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --jars=./libSedona/* --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} AREAWATER ROADS 960 94000000 276000000
    done

    for i in 0.012 0.009 0.006 0.003; do
      spark-submit --class gr.ds.unipi.epsilongrid.MetricsReals --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 7g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m --conf spark.sql.shuffle.partitions=720 --conf spark.default.parallelism=300 ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} AREAWATER LINEARWATER
      spark-submit --class gr.ds.unipi.twoEpsilongrid.MetricsReals --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 7g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m --conf spark.sql.shuffle.partitions=720 --conf spark.default.parallelism=300 ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} AREAWATER LINEARWATER
      spark-submit --class gr.ds.unipi.rdd.MetricsRealsPruned --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} 4 AREAWATER LINEARWATER 960 DIAG_PR 0.001
      spark-submit --class gr.ds.unipi.processing.MetricsRealsPruned --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} AREAWATER LINEARWATER 960 0.001
      if [ "$i" != "0.012" ]; then
      spark-submit --class gr.ds.unipi.stripes.MetricsRealsX --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} 4 AREAWATER LINEARWATER 960 DIAG_PR 0.001
      fi
      spark-submit --class gr.ds.unipi.sedona.Metrics --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --jars=./libSedona/* --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} AREAWATER LINEARWATER 960 94000000 278000000
    done

    for i in 0.012 0.009 0.006 0.003; do
      spark-submit --class gr.ds.unipi.epsilongrid.MetricsReals --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 7g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m --conf spark.sql.shuffle.partitions=720 --conf spark.default.parallelism=300 ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} ROADS LINEARWATER
      spark-submit --class gr.ds.unipi.twoEpsilongrid.MetricsReals --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 7g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m --conf spark.sql.shuffle.partitions=720 --conf spark.default.parallelism=300 ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} ROADS LINEARWATER
      spark-submit --class gr.ds.unipi.rdd.MetricsRealsPruned --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} 4 ROADS LINEARWATER 960 DIAG_PR 0.001
      spark-submit --class gr.ds.unipi.processing.MetricsRealsPruned --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} ROADS LINEARWATER 960 0.001
      spark-submit --class gr.ds.unipi.stripes.MetricsRealsX --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} 4 ROADS LINEARWATER 960 DIAG_PR 0.001
      spark-submit --class gr.ds.unipi.sedona.Metrics --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --jars=./libSedona/* --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} ROADS LINEARWATER 960 276000000 278000000
    done


    for i in 0.012 0.009 0.006 0.003; do
      spark-submit --class gr.ds.unipi.epsilongrid.MetricsReals --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 7g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m --conf spark.sql.shuffle.partitions=720 --conf spark.default.parallelism=300 ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} PARKS BUILDINGS
      spark-submit --class gr.ds.unipi.twoEpsilongrid.MetricsReals --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 7g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m --conf spark.sql.shuffle.partitions=720 --conf spark.default.parallelism=300 ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} PARKS BUILDINGS
      spark-submit --class gr.ds.unipi.rdd.MetricsRealsPruned --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} 4 PARKS BUILDINGS 960 DIAG_PR 0.001
      spark-submit --class gr.ds.unipi.processing.MetricsRealsPruned --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} PARKS BUILDINGS 960 0.001
      spark-submit --class gr.ds.unipi.stripes.MetricsRealsX --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} 4 PARKS BUILDINGS 960 DIAG_PR 0.001
      spark-submit --class gr.ds.unipi.sedona.Metrics --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --jars=./libSedona/* --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} PARKS BUILDINGS 960 42700000 58000000
    done


    for i in 0.012 0.009 0.006 0.003; do
      spark-submit --class gr.ds.unipi.epsilongrid.MetricsReals --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 7g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m --conf spark.sql.shuffle.partitions=720 --conf spark.default.parallelism=300 ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} PARKS ROADS
      spark-submit --class gr.ds.unipi.twoEpsilongrid.MetricsReals --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 7g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m --conf spark.sql.shuffle.partitions=720 --conf spark.default.parallelism=300 ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} PARKS ROADS
      spark-submit --class gr.ds.unipi.rdd.MetricsRealsPruned --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} 4 PARKS ROADS 960 DIAG_PR 0.001
      spark-submit --class gr.ds.unipi.processing.MetricsRealsPruned --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} PARKS ROADS 960 0.001
      spark-submit --class gr.ds.unipi.stripes.MetricsRealsX --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} 4 PARKS ROADS 960 DIAG_PR 0.001
      spark-submit --class gr.ds.unipi.sedona.Metrics --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --jars=./libSedona/* --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} PARKS ROADS 960 276000000 278000000
    done

    for i in 0.012 0.009 0.006 0.003; do
      spark-submit --class gr.ds.unipi.epsilongrid.MetricsReals --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 7g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m --conf spark.sql.shuffle.partitions=720 --conf spark.default.parallelism=300 ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} BUILDINGS ROADS
      spark-submit --class gr.ds.unipi.twoEpsilongrid.MetricsReals --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 7g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m --conf spark.sql.shuffle.partitions=720 --conf spark.default.parallelism=300 ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} BUILDINGS ROADS
      spark-submit --class gr.ds.unipi.rdd.MetricsRealsPruned --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} 4 BUILDINGS ROADS 960 DIAG_PR 0.001
      spark-submit --class gr.ds.unipi.processing.MetricsRealsPruned --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} BUILDINGS ROADS 960 0.001
      spark-submit --class gr.ds.unipi.stripes.MetricsRealsX --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} 4 BUILDINGS ROADS 960 DIAG_PR 0.001
      spark-submit --class gr.ds.unipi.sedona.Metrics --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --jars=./libSedona/* --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} BUILDINGS ROADS 960 58700000 276000000
    done

    for i in 0.012 0.009 0.006 0.003; do
      spark-submit --class gr.ds.unipi.epsilongrid.MetricsReals --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 7g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m --conf spark.sql.shuffle.partitions=720 --conf spark.default.parallelism=300 ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} BUILDINGS LINEARWATER
      spark-submit --class gr.ds.unipi.twoEpsilongrid.MetricsReals --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 7g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m --conf spark.sql.shuffle.partitions=720 --conf spark.default.parallelism=300 ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} BUILDINGS LINEARWATER
      spark-submit --class gr.ds.unipi.rdd.MetricsRealsPruned --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} 4 BUILDINGS LINEARWATER 960 DIAG_PR 0.001
      spark-submit --class gr.ds.unipi.processing.MetricsRealsPruned --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} BUILDINGS LINEARWATER 960 0.001
      spark-submit --class gr.ds.unipi.stripes.MetricsRealsX --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} 4 BUILDINGS LINEARWATER 960 DIAG_PR 0.001
      spark-submit --class gr.ds.unipi.sedona.Metrics --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --jars=./libSedona/* --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} BUILDINGS LINEARWATER 960 58700000 278000000
    done



for i in 0.012 0.009 0.006 0.003; do
  spark-submit --class gr.ds.unipi.epsilongrid.MetricsReals --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 7g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m --conf spark.sql.shuffle.partitions=720 --conf spark.default.parallelism=300 ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} AREAWATER datasetA100000000
  spark-submit --class gr.ds.unipi.twoEpsilongrid.MetricsReals --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 7g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m --conf spark.sql.shuffle.partitions=720 --conf spark.default.parallelism=300 ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} AREAWATER datasetA100000000
  spark-submit --class gr.ds.unipi.rdd.MetricsRealsPruned --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} 4 AREAWATER datasetA100000000 960 DIAG_PR 0.001
  spark-submit --class gr.ds.unipi.processing.MetricsRealsPruned --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} AREAWATER datasetA100000000 960 0.001
  spark-submit --class gr.ds.unipi.stripes.MetricsRealsX --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} 4 AREAWATER datasetA100000000 960 DIAG_PR 0.001
done

for i in 0.012 0.009 0.006 0.003; do
  spark-submit --class gr.ds.unipi.epsilongrid.MetricsReals --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 7g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m --conf spark.sql.shuffle.partitions=720 --conf spark.default.parallelism=300 ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} AREAWATER datasetA200000000
  spark-submit --class gr.ds.unipi.twoEpsilongrid.MetricsReals --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 7g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m --conf spark.sql.shuffle.partitions=720 --conf spark.default.parallelism=300 ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} AREAWATER datasetA200000000
  spark-submit --class gr.ds.unipi.rdd.MetricsRealsPruned --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} 4 AREAWATER datasetA200000000 960 DIAG_PR 0.001
  spark-submit --class gr.ds.unipi.processing.MetricsRealsPruned --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} AREAWATER datasetA200000000 960 0.001
  if [ "$i" != "0.012" ]; then
  spark-submit --class gr.ds.unipi.stripes.MetricsRealsX --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} 4 AREAWATER datasetA200000000 960 DIAG_PR 0.001
  fi
done


for i in 0.012 0.009 0.006 0.003; do
  spark-submit --class gr.ds.unipi.epsilongrid.MetricsReals --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 7g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m --conf spark.sql.shuffle.partitions=720 --conf spark.default.parallelism=300 ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} datasetA200000000 datasetB200000000
  spark-submit --class gr.ds.unipi.twoEpsilongrid.MetricsReals --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 7g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m --conf spark.sql.shuffle.partitions=720 --conf spark.default.parallelism=300 ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} datasetA200000000 datasetB200000000
  spark-submit --class gr.ds.unipi.rdd.MetricsRealsPruned --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} 4 datasetA200000000 datasetB200000000 960 DIAG_PR 0.001
  spark-submit --class gr.ds.unipi.processing.MetricsRealsPruned --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} datasetA200000000 datasetB200000000 960 0.001
  if [ "$i" != "0.012" ]; then
    spark-submit --class gr.ds.unipi.stripes.MetricsRealsX --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ ${i} 4 datasetA200000000 datasetB200000000 960 DIAG_PR 0.001
  fi
done


for i in 1000000000 800000000 600000000 400000000; do
  if [ "$i" -eq 400000000 ]; then
      spark-submit --class gr.ds.unipi.epsilongrid.MetricsReals --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 7g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m --conf spark.sql.shuffle.partitions=720 --conf spark.default.parallelism=300 ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ 0.006 datasetA${i} datasetB${i}
  fi
  spark-submit --class gr.ds.unipi.twoEpsilongrid.MetricsReals --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 7g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m --conf spark.sql.shuffle.partitions=720 --conf spark.default.parallelism=300 ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ 0.006 datasetA${i} datasetB${i}
  spark-submit --class gr.ds.unipi.rdd.MetricsRealsPruned --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ 0.006 4 datasetA${i} datasetB${i} 960 DIAG_PR 0.001
  spark-submit --class gr.ds.unipi.processing.MetricsRealsPruned --master yarn --deploy-mode client --conf spark.yarn.archive="hdfs://node13:9000/user/spark-jars.tar.gz" --executor-memory 5g --executor-cores 2 --driver-memory 6g --num-executors 12 --conf spark.network.timeout=1000000s --conf spark.shuffle.io.maxRetries=10 --conf spark.executor.memoryOverhead=1g --conf spark.kryoserializer.buffer.max=2047m ~/Spatial-Joins-1.0-SNAPSHOT.jar hdfs://node13:9000/user/user/synth-skewed/pruned/ 0.006 datasetA${i} datasetB${i} 960 0.001
done
